{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incorrect-extraction",
   "metadata": {},
   "source": [
    "# Task IV - Graph Neural Network (GNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-franchise",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "### Goal\n",
    "\n",
    "- Perform Quark/Gluon jet classification on ParticleNetâ€™s data for using Graph Neural Network (GNN)\n",
    "\n",
    "### On the packages used\n",
    "\n",
    "- In this task, we wil be using the `dgl` (deep graph library) package for constructing GNN models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naval-banking",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "module functions cannot set METH_CLASS or METH_STATIC",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7d70955a554b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/afs/cern.ch/work/c/chlcheng/public/local/conda/miniconda/envs/ml-base/lib/python3.7/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"
     ]
    }
   ],
   "source": [
    "# load some basic packages first\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-casting",
   "metadata": {},
   "source": [
    "# 2. Create Graph Representation of Data\n",
    "\n",
    "References:\n",
    "\n",
    "How to create a graph  https://docs.dgl.ai/tutorials/basics/1_first.html\n",
    "\n",
    "How to assign features to edges:  https://docs.dgl.ai/guide/training-edge.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-asthma",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "The dataset contains:\n",
    "\n",
    "- 50k quark and 50k gluons with constitutes (charged or neural tracks) of the jets\n",
    "\n",
    "- Each jet is represented by a four-dimensional vector: $p_T, y, \\phi$, and PDGid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "guilty-auckland",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "KeyboardInterrupt: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8a719b07ba99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/QG_orig/QG_jets.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/work/c/chlcheng/public/local/conda/miniconda/envs/ml-base/lib/python3.7/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: KeyboardInterrupt: "
     ]
    }
   ],
   "source": [
    "\n",
    "data = np.load('../data/QG_orig/QG_jets.npz')\n",
    "X, y = data['X'], {'label': torch.tensor(data['y']).long()}\n",
    "print('Dimension of input data: {}'.format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-jesus",
   "metadata": {},
   "source": [
    "A graph can have features in three different places or levels:\n",
    "\n",
    "- features for nodes\n",
    "- features for edges\n",
    "- features for the whole graph\n",
    "\n",
    "A **jet** is **encoded as a graph**.\n",
    "\n",
    "The **label** of whether it is a quark or a gluon is **encoded as the graph feature**.\n",
    "\n",
    "Each constitute (**track**) is encoded as a **node** of the graph, with the **four-dimensional vector** as its **node features**.\n",
    "\n",
    "Features are preprocessed to reflect a centred jets and normalised $p_T$ according to Eqn 3.1 of the original paper (https://arxiv.org/abs/1810.05165).\n",
    "\n",
    "**Nodes** are **connected by bi-directional edges** if they are **adjacent** in either $p_T, y, \\phi$.\n",
    "\n",
    "This ensures that each graph is connected.\n",
    "\n",
    "**Nodes index** are **ordered in descending order of $p_T$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-healing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create graphs and save to local disk '''\n",
    "        input_data = np.load(input_file)\n",
    "        X, y = input_data['X'], {'label': torch.tensor(input_data['y']).long()}\n",
    "        \n",
    "    graph_file = f'../data/QG_graph/QG_jets__{args.connection}__{args.nevent if args.nevent else \"All\"}.bin'\n",
    "    gmodel_name = f'../data/QG_model/QG_jets.model'        \n",
    "        generator = GenerateGraphs(X, args.connection)\n",
    "        graphs = generator.create_graphs(stop=args.nevent)\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mCreate graphs from\\033[0m'.rjust(40, ' '), input_file, len(graphs))\n",
    "        makedirs(path.dirname(graph_file), exist_ok=True)\n",
    "        save_graphs(graph_file, graphs, y)\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mSave graphs to\\033[0m'.rjust(40, ' '), graph_file, len(graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "# https://github.com/dmlc/xgboost/issues/1715\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# data = X # X.shape = (njets, track multiplicity, 4-momentum)\n",
    "\n",
    "class GenerateGraphs(object):\n",
    "    def describe(self): return self.__class__.__name__\n",
    "    def __init__(self, data, connect_style):\n",
    "        self.data = data\n",
    "        self.label_name = 'label'\n",
    "        self.feature_name = 'feature'\n",
    "        self.connect_style = connect_style\n",
    "        self._current_data = None\n",
    "\n",
    "    def _assign_node_feature(self, graph):\n",
    "        ''' Each node presents a b-jet with four momentum and b-tagging score as features.\n",
    "        '''\n",
    "        feature_name = 'feature'\n",
    "        assert(self._current_data is not None)\n",
    "        graph.ndata[feature_name] = torch.tensor(self._current_data)\n",
    "        return graph\n",
    "\n",
    "    def _create_graph(self, ievent: int):\n",
    "        ''' Create a graph from a jet X [track multiplicity, \"4-momentum\"], where \"4-momentum\" is pt, rapidity, azimuthal angle, and pdgid.\n",
    "            data: https://zenodo.org/record/3164691#.YFeQey1Q0lp\n",
    "        '''\n",
    "        self._current_data = self.data[ievent][~np.all(self.data[ievent] == 0, axis=1)]\n",
    "\n",
    "        ''' Feature preprocessing: Sect 3.1 in https://arxiv.org/pdf/1810.05165.pdf\n",
    "            centering jets and normalizing pT\n",
    "            https://energyflow.network/examples/\n",
    "        '''\n",
    "        yphi_avg = np.average(self._current_data[:,1:3], weights=self._current_data[:,0], axis=0)\n",
    "        self._current_data[:,1:3] -= yphi_avg\n",
    "        self._current_data[:, 0] /= np.sum(self._current_data[:, 0])\n",
    "\n",
    "        ''' - Sort by pT (0th column) '''\n",
    "        self._current_data = self._current_data[self._current_data[:,0].argsort()][::-1].copy()\n",
    "\n",
    "        ''' Construct a graph '''\n",
    "        n_nodes = self._current_data.shape[0] # track multiplicity\n",
    "        if self.connect_style == 'bifully':\n",
    "            ''' Option 1: Fully connected graph '''\n",
    "            g = nx.complete_graph(n_nodes)\n",
    "            graph = dgl.from_networkx(g)\n",
    "        else:\n",
    "            ''' Option 2: Bi-directional connection in adjacent track in pT or eta or phi'''\n",
    "            pt_order = self._current_data[:,0].argsort()[::-1]\n",
    "            rapidity_order = self._current_data[:,1].argsort()[::-1]\n",
    "            eta_order = self._current_data[:,2].argsort()[::-1]\n",
    "\n",
    "            if self.connect_style == 'biadj_pt_y_phi':\n",
    "                in_node  = np.concatenate((pt_order[:-1], rapidity_order[:-1], eta_order[:-1]))\n",
    "                out_node = np.concatenate((pt_order[1: ], rapidity_order[1: ], eta_order[1: ]))\n",
    "            elif self.connect_style == 'biadj_pt_y':\n",
    "                in_node  = np.concatenate((pt_order[:-1], rapidity_order[:-1]))\n",
    "                out_node = np.concatenate((pt_order[1: ], rapidity_order[1: ]))\n",
    "            elif self.connect_style == 'biadj_pt_phi':\n",
    "                in_node  = np.concatenate((pt_order[:-1], eta_order[:-1]))\n",
    "                out_node = np.concatenate((pt_order[1: ], eta_order[1: ]))\n",
    "            elif self.connect_style == 'biadj_y_phi':\n",
    "                in_node  = np.concatenate((rapidity_order[:-1], eta_order[:-1]))\n",
    "                out_node = np.concatenate((rapidity_order[1: ], eta_order[1: ]))\n",
    "\n",
    "            g = dgl.graph(( in_node, out_node), num_nodes=n_nodes)\n",
    "            g = dgl.add_reverse_edges(g)\n",
    "            graph = dgl.add_self_loop(g)\n",
    "\n",
    "\n",
    "        ''' Assign node feature using \"current data\" '''\n",
    "        graph = self._assign_node_feature(graph)\n",
    "\n",
    "        return graph.int() # 32-bit integers for node and edge IDs to reduce memory\n",
    "\n",
    "    def create_graphs(self, stop=None):\n",
    "        ''' Create all graphs for all events.\n",
    "        '''\n",
    "\n",
    "        ''' PDGid to small float dictionary https://github.com/pkomiske/EnergyFlow/blob/master/energyflow/utils/data_utils.py#L188 '''\n",
    "        PID2FLOAT_MAP = {22: 0,\n",
    "                    211: .1, -211: .2,\n",
    "                    321: .3, -321: .4,\n",
    "                    130: .5,\n",
    "                    2112: .6, -2112: .7,\n",
    "                    2212: .8, -2212: .9,\n",
    "                    11: 1.0, -11: 1.1,\n",
    "                    13: 1.2, -13: 1.3,\n",
    "                    0: 0,}\n",
    "        for pid in np.unique(self.data[:, :, 3].flatten()):\n",
    "            np.place(self.data[:, :, 3], self.data[:, :, 3] == pid, PID2FLOAT_MAP[pid])\n",
    "\n",
    "        graphs = []\n",
    "        n_graphs = min(stop, self.data.shape[0]) if stop else self.data.shape[0]\n",
    "        \n",
    "        for i in range(n_graphs):\n",
    "            if i % 1000 == 0:\n",
    "                print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', f'\\033[92mCreated {self.connect_style} graphs:\\033[0m'.rjust(40, ' '),  i, '/', n_graphs)\n",
    "            graph = self._create_graph(i)\n",
    "            graphs.append(graph)\n",
    "        return graphs\n",
    "\n",
    "\n",
    "def plot_graph(graph):\n",
    "    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mNumber of nodes and edges:\\033[0m'.rjust(40, ' '),  graph.number_of_nodes(), '/', graph.number_of_edges())\n",
    "    nx.draw(graph.to_networkx(), with_labels=True, node_color=[[.7, .7, .7]])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-newman",
   "metadata": {},
   "source": [
    "## Implementing Various GNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "import dgl\n",
    "import numpy as np\n",
    "from dgl.nn.pytorch.conv import GraphConv\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "class SGC(nn.Module):\n",
    "    ''' \n",
    "        https://docs.dgl.ai/api/python/nn.pytorch.html?highlight=sageconv#sgconv\n",
    "    '''\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.SGConv(in_feats=in_feats, out_feats=hid_feats)\n",
    "        self.conv2 = dglnn.SGConv(in_feats=hid_feats, out_feats=out_feats)\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = F.elu(h)\n",
    "        h = self.conv2(graph, h)\n",
    "        h = torch.sigmoid(h)\n",
    "        return h\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    ''' Two layers of Inductive Representation Learning on Large Graphs.\n",
    "        https://docs.dgl.ai/api/python/nn.pytorch.html#sageconv\n",
    "    '''\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.SAGEConv(in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')\n",
    "        self.conv2 = dglnn.SAGEConv(in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = F.elu(h)\n",
    "        h = self.conv2(graph, h)\n",
    "        h = torch.sigmoid(h)\n",
    "        return h\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    ''' Two layers of Graph Attention Network.\n",
    "        https://docs.dgl.ai/api/python/nn.pytorch.html#gatconv\n",
    "    '''\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        # input shape = (nodes, features=in_feats); output shape = (nodes, num_head, hid_feats)\n",
    "        self.gatconv1 = dglnn.GATConv(in_feats, hid_feats, num_heads=2)\n",
    "        # input shape = (nodes, hid_feats * num_heads_previous_layer); output shape = (nodes, num_head, out_feats)\n",
    "        self.gatconv2 = dglnn.GATConv(hid_feats * 2, out_feats, num_heads=1)\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # input shape = (nodes , features)\n",
    "        # print('zhangr inputs', inputs.shape, inputs)\n",
    "        h = self.gatconv1(graph, inputs)\n",
    "        # here h shape = (nodes, num_head, hid_feats)\n",
    "        # print('zhangr h1', h.shape, h)\n",
    "        ''' Reshape h to flatten the num_heads '''\n",
    "        # here h shape = (nodes, num_head * hid_feats)\n",
    "        h = h.reshape(h.shape[0], np.prod(h.shape[1:]))\n",
    "        # print('zhangr h2', h.shape, h)\n",
    "        h = self.gatconv2(graph, h)\n",
    "        # print('zhangr h3', h.shape, h)\n",
    "\n",
    "        graph.ndata['tmp_feature'] = h\n",
    "        h = dgl.mean_nodes(graph, 'tmp_feature')\n",
    "        # print('zhangr h4', h.shape, h)\n",
    "        h = torch.sigmoid(h)\n",
    "        h = torch.squeeze(h)\n",
    "        # print('zhangr h9', h.shape, h)\n",
    "        return h\n",
    "\n",
    "class AGNNConv(nn.Module):\n",
    "    ''' Graph Attention Network.\n",
    "        https://docs.dgl.ai/api/python/nn.pytorch.html#agnnconv\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = AGNNConv()(graph, inputs)\n",
    "        h = torch.sigmoid(h)\n",
    "        return h\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    ''' Graph Convolutional Network\n",
    "        https://docs.dgl.ai/en/0.4.x/tutorials/basics/4_batch.html#graph-classification-tutorial\n",
    "        https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/1_gcn.html\n",
    "    '''\n",
    "    def __init__(self, in_dim, hidden_dim, out_feats):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, out_feats)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=out_feats)\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        # Use node degree as the initial node feature. For undirected graphs, the in-degree\n",
    "        # is the same as the out_degree.\n",
    "        # h = graph.in_degrees().view(-1, 1).float()\n",
    "        # Perform graph convolution and activation function.\n",
    "        h = F.relu(self.bn1(self.conv1(graph, h)))\n",
    "        h = F.relu(self.bn2(self.conv2(graph, h)))\n",
    "        h = F.relu(self.bn3(self.conv3(graph, h)))\n",
    "        graph.ndata['tmp_feature'] = h\n",
    "        h = dgl.mean_nodes(graph, 'tmp_feature')\n",
    "        # Calculate graph representation by averaging all the node representations.\n",
    "        h = torch.sigmoid(h)\n",
    "        h = (h-0.5) * 2\n",
    "        return h\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features * 2, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        score = self.W(torch.cat([h_u, h_v], 1))\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        # h contains the node representations computed from the GNN defined\n",
    "        # in the node classification section (Section 5.1).\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']\n",
    "\n",
    "class GNNmodel(nn.Module):\n",
    "    def __init__(self, args, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mGNN module\\033[0m'.rjust(40, ' '), args.module.lower())\n",
    "        if args.module.lower() == 'sage':\n",
    "            # from dgl.nn import SAGEConv\n",
    "            self.module = SAGE(in_features, hidden_features, out_features)\n",
    "        elif args.module.lower() == 'gat':\n",
    "            # from dgl.nn import GATConv\n",
    "            self.module = GAT(in_features, hidden_features, out_features)\n",
    "        elif args.module.lower() == 'agnnconv':\n",
    "            # from dgl.nn import AGNNConv\n",
    "            self.module = AGNNConv()\n",
    "        elif args.module.lower() == 'sgc':\n",
    "            # from dgl.nn import SGConv\n",
    "            self.module = SGC(in_features, hidden_features, out_features)\n",
    "        elif args.module.lower() == 'gcn':\n",
    "            self.module = GCN(in_features, hidden_features, out_features)\n",
    "        else:\n",
    "            assert False, args.module + ' does not support'\n",
    "\n",
    "        # self.pred = MLPPredictor(out_features, 1)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        h = self.module(g, x)\n",
    "        return torch.squeeze(h) #self.pred(g, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "def cal_acc(test_target, test_yhat, train_target, train_yhat):\n",
    "    train_results = np.logical_not(np.logical_xor(train_target.numpy() > 0.5, train_yhat> 0.5))\n",
    "    test_results = np.logical_not(np.logical_xor(test_target.numpy() > 0.5, test_yhat> 0.5))\n",
    "    train_acc = np.count_nonzero(train_results) / len(train_results)\n",
    "    test_acc = np.count_nonzero(test_results) / len(test_results)\n",
    "    \n",
    "    return test_acc, train_acc\n",
    "\n",
    "def plot_ROC(test_y, test_predict, train_y = None, train_predict = None, val_y = None, val_predict = None, show = True):\n",
    "    if train_y is not None and train_predict is not None:\n",
    "        train__false_positive, train__true_positive, train__thresholds = roc_curve(train_y, train_predict)\n",
    "        train__roc_auc = auc(train__false_positive, train__true_positive)\n",
    "    if val_y is not None and val_predict is not None:\n",
    "        val__false_positive, val__true_positive, val__thresholds = roc_curve(val_y, val_predict)\n",
    "        val__roc_auc = auc(val__false_positive, val__true_positive)\n",
    "    test__false_positive, test__true_positive, test__thresholds = roc_curve(test_y, test_predict)\n",
    "    test__roc_auc = auc(test__false_positive, test__true_positive)\n",
    "\n",
    "    if show:\n",
    "        # plt.title('Receiver Operating Characteristic')\n",
    "        if train_y is not None and train_predict is not None:\n",
    "            plt.plot(train__true_positive, 1-train__false_positive, 'g--', label='Train AUC = %0.3f'% train__roc_auc)\n",
    "        if val_y is not None and val_predict is not None:\n",
    "            plt.plot(val__true_positive, 1-val__false_positive, 'b--', label='Val AUC = %0.3f'% val__roc_auc)\n",
    "        plt.plot(test__true_positive, 1-test__false_positive, 'r', label='Test AUC = %0.3f'% test__roc_auc)\n",
    "\n",
    "        plt.legend(loc='lower left')\n",
    "        plt.plot([0,1],[1,0],'k--')\n",
    "        plt.xlim([-0.1,1.1])\n",
    "        plt.ylim([-0.1,1.1])\n",
    "        plt.ylabel('Background rejection')\n",
    "        plt.xlabel('Signal efficiency')\n",
    "        plt.show()\n",
    "\n",
    "    if train_y is not None and train_predict is not None:\n",
    "        return test__roc_auc, train__roc_auc\n",
    "    else:\n",
    "        return test__roc_auc\n",
    "\n",
    "def plotPRC(test_y, test_predict, train_y = None, train_predict = None, show = True):\n",
    "    if train_y is not None and train_predict is not None:\n",
    "        train__precision, train__recall, train__thresholds = precision_recall_curve(train_y, train_predict)\n",
    "        train__prec_auc = auc(train__recall, train__precision)\n",
    "    test__precision, test__recall, test__thresholds = precision_recall_curve(test_y, test_predict)\n",
    "    test__prec_auc = auc(test__recall, test__precision)\n",
    "\n",
    "    if show:\n",
    "        plt.title('Precision-Recall Curves')\n",
    "        if train_y is not None and train_predict is not None:\n",
    "            plt.plot(train__precision, train__recall, 'g--', label='Train PRC = %0.3f'% train__prec_auc)\n",
    "        plt.plot(test__precision, test__recall, 'b', label='Test PRC = %0.3f'% test__prec_auc)\n",
    "\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.xlim([-0.,1.1])\n",
    "        plt.ylim([-0.,1.1])\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.show()\n",
    "\n",
    "    if train_y is not None and train_predict is not None:\n",
    "        return test__prec_auc, train__prec_auc\n",
    "    else:\n",
    "        return test__prec_auc\n",
    "\n",
    "def plot_response(test_y, test_predict, train_y = None, train_predict = None, val_y = None, val_predict = None, nbin = 10, normalised = True, log = False, scaling = ''):\n",
    "    xlo, xhi = 0, 1\n",
    "\n",
    "    if scaling == 'minmax':\n",
    "        hi, lo = max(test_predict), min(test_predict)\n",
    "        if hi != lo:\n",
    "            test_predict = (test_predict - lo) / (hi - lo)\n",
    "\n",
    "    plt.hist(test_predict[test_y == 1], range=[xlo, xhi], bins=nbin, histtype=\"step\", density=normalised, linewidth=2, label='Test signal')\n",
    "    plt.hist(test_predict[test_y == 0], range=[xlo, xhi], bins=nbin, histtype=\"step\", density=normalised, linewidth=2, label='Test bkg')\n",
    "    if train_y is not None and train_predict is not None:\n",
    "        if scaling == 'minmax':\n",
    "            if hi != lo:\n",
    "                train_predict = (train_predict - lo) / (hi - lo)\n",
    "        plt.hist(train_predict[train_y == 1], range=[xlo, xhi], bins=nbin, histtype=\"step\", density=normalised, linewidth=2, linestyle='dashed', label='Training signal')\n",
    "        plt.hist(train_predict[train_y == 0], range=[xlo, xhi], bins=nbin, histtype=\"step\", density=normalised, linewidth=2, linestyle='dashed', label='Training bkg')\n",
    "    if val_y  is not None and val_predict is not None:\n",
    "        if scaling == 'minmax':\n",
    "            if hi != lo:\n",
    "                val_predict = (val_predict - lo) / (hi - lo)\n",
    "        plt.hist(val_predict[train_y == 1], range=[xlo, xhi], bins=nbin, histtype=\"step\", density=normalised, linewidth=1, linestyle='dashed', label='Val signal')\n",
    "        plt.hist(val_predict[train_y == 0], range=[xlo, xhi], bins=nbin, histtype=\"step\", density=normalised, linewidth=1, linestyle='dashed', label='Val bkg')\n",
    "    plt.ylim(0, plt.gca().get_ylim()[1] * 1.5)\n",
    "    if log:\n",
    "        plt.yscale('symlog')\n",
    "        plt.ylabel('[Log scale]')\n",
    "    plt.legend(loc='best', fancybox=True, framealpha=0.2)\n",
    "    plt.xlabel('Response', fontsize='large')\n",
    "    plt.title('Normalised' if normalised else 'Absolute')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(train_loss, valid_loss, starting = 0):\n",
    "    # summarize history for loss\n",
    "    plt.plot(train_loss[starting:])\n",
    "    plt.plot(valid_loss[starting:])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# https://github.com/sksq96/pytorch-summary/blob/master/torchsummary/torchsummary.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def summary(model, input_size, batch_size=-1, dtypes=None):\n",
    "    result, params_info = summary_string(\n",
    "        model, input_size, batch_size, dtypes)\n",
    "    print(result)\n",
    "\n",
    "    return params_info\n",
    "\n",
    "\n",
    "def summary_string(model, input_size, batch_size=-1, dtypes=None):\n",
    "    if dtypes == None:\n",
    "        dtypes = [torch.FloatTensor]*len(input_size)\n",
    "\n",
    "    summary_str = ''\n",
    "\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "\n",
    "            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    # multiple inputs to the network\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    # batch_size of 2 for batchnorm\n",
    "    x = [torch.rand(2, *in_size).type(dtype)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    # create properties\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # make a forward pass\n",
    "    # print(x.shape)\n",
    "    model(*x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "        \"Layer (type)\", \"Output Shape\", \"Param #\")\n",
    "    summary_str += line_new + \"\\n\"\n",
    "    summary_str += \"================================================================\" + \"\\n\"\n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    for layer in summary:\n",
    "        # input_shape, output_shape, trainable, nb_params\n",
    "        line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "            layer,\n",
    "            str(summary[layer][\"output_shape\"]),\n",
    "            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "        )\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "\n",
    "        total_output += np.prod(summary[layer][\"output_shape\"])\n",
    "        if \"trainable\" in summary[layer]:\n",
    "            if summary[layer][\"trainable\"] == True:\n",
    "                trainable_params += summary[layer][\"nb_params\"]\n",
    "        summary_str += line_new + \"\\n\"\n",
    "\n",
    "    # assume 4 bytes/number (float on cuda).\n",
    "    total_input_size = abs(np.prod(sum(input_size, ()))\n",
    "                           * batch_size * 4. / (1024 ** 2.))\n",
    "    total_output_size = abs(2. * total_output * 4. /\n",
    "                            (1024 ** 2.))  # x2 for gradients\n",
    "    total_params_size = abs(total_params * 4. / (1024 ** 2.))\n",
    "    total_size = total_params_size + total_output_size + total_input_size\n",
    "\n",
    "    summary_str += \"================================================================\" + \"\\n\"\n",
    "    summary_str += \"Total params: {0:,}\".format(total_params) + \"\\n\"\n",
    "    summary_str += \"Trainable params: {0:,}\".format(trainable_params) + \"\\n\"\n",
    "    summary_str += \"Non-trainable params: {0:,}\".format(total_params -\n",
    "                                                        trainable_params) + \"\\n\"\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    summary_str += \"Input size (MB): %0.2f\" % total_input_size + \"\\n\"\n",
    "    summary_str += \"Forward/backward pass size (MB): %0.2f\" % total_output_size + \"\\n\"\n",
    "    summary_str += \"Params size (MB): %0.2f\" % total_params_size + \"\\n\"\n",
    "    summary_str += \"Estimated Total Size (MB): %0.2f\" % total_size + \"\\n\"\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    # return summary\n",
    "    return summary_str, (total_params, trainable_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-transport",
   "metadata": {},
   "source": [
    "## Do the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "import random\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "from os import makedirs, path, remove\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dgl\n",
    "from dgl.data.utils import save_graphs, load_graphs\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from createGraph import *\n",
    "from createGNN import *\n",
    "from pytorchtools import *\n",
    "from plottools import *\n",
    "\n",
    "from matplotlib import rcParams\n",
    "size = 15\n",
    "rcParams['font.size'] = size\n",
    "rcParams['font.weight'] = 'bold'\n",
    "rcParams['axes.labelweight'] = 'bold'\n",
    "rcParams['axes.titleweight'] = 'bold'\n",
    "rcParams['axes.linewidth'] = 2\n",
    "rcParams['figure.facecolor'] = (1,1,1,0)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "''' How to batch graphs '''\n",
    "# https://docs.dgl.ai/en/0.4.x/tutorials/basics/4_batch.html\n",
    "\n",
    "\n",
    "def train_q_g(args):\n",
    "\n",
    "    input_file = f'../data/QG_orig/QG_jets.npz'\n",
    "    graph_file = f'../data/QG_graph/QG_jets__{args.connection}__{args.nevent if args.nevent else \"All\"}.bin'\n",
    "    gmodel_name = f'../data/QG_model/QG_jets.model'\n",
    "\n",
    "    makedirs(path.dirname(gmodel_name), exist_ok=True)\n",
    "\n",
    "    if not args.overwrite and path.exists(graph_file):\n",
    "        ''' Load graphs from local disk '''\n",
    "        graphs, y = load_graphs(graph_file)\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mLoad training graphs from\\033[0m'.rjust(40, ' '), graph_file, len(graphs))\n",
    "        if args.nevent:\n",
    "            graphs = graphs[:args.nevent]\n",
    "    else:\n",
    "        ''' Create graphs and save to local disk '''\n",
    "        input_data = np.load(input_file)\n",
    "        X, y = input_data['X'], {'label': torch.tensor(input_data['y']).long()}\n",
    "        generator = GenerateGraphs(X, args.connection)\n",
    "        graphs = generator.create_graphs(stop=args.nevent)\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mCreate graphs from\\033[0m'.rjust(40, ' '), input_file, len(graphs))\n",
    "        makedirs(path.dirname(graph_file), exist_ok=True)\n",
    "        save_graphs(graph_file, graphs, y)\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mSave graphs to\\033[0m'.rjust(40, ' '), graph_file, len(graphs))\n",
    "\n",
    "    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mBelow the 1st graphs\\033[0m'.rjust(40, ' '))\n",
    "    plot_graph(graphs[0])\n",
    "\n",
    "    ''' Construct dataset / dataloader '''\n",
    "    train_size = int(len(graphs) * 0.6)\n",
    "    val_size = int(len(graphs) * 0.2)\n",
    "    test_size = len(graphs) - train_size - val_size\n",
    "\n",
    "    dataset = list(zip(graphs, y['label']))\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_size, val_size, test_size))\n",
    "\n",
    "    train_dataloader = dgl.dataloading.GraphDataLoader(train_dataset, batch_size = train_size, drop_last=False, shuffle=True)\n",
    "    val_dataloader = dgl.dataloading.GraphDataLoader(val_dataset, batch_size = val_size, drop_last=False, shuffle=False)\n",
    "    test_dataloader = dgl.dataloading.GraphDataLoader(test_dataset, batch_size = test_size, drop_last=False, shuffle=False)\n",
    "\n",
    "    # for g in train_graphs:\n",
    "    #     print('zhangr train graph', g.ndata, g.nodes())\n",
    "    # for g in val_graphs:\n",
    "    #     print('zhangr val graph', g.ndata, g.nodes())\n",
    "    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mSplit train/val/test to\\033[0m'.rjust(40, ' '), f'{train_size}/{val_size}/{test_size}')\n",
    "    loss_func = nn.MSELoss() # nn.CrossEntropyLoss()\n",
    "\n",
    "    if not args.no_training:\n",
    "        ''' Construct training model '''\n",
    "        hidden_features, out_features = 9, 1 # 2 = classifier predict\n",
    "        model = GNNmodel(args, graphs[0].ndata[list(graphs[0].ndata.keys())[0]].shape[1], hidden_features, out_features)\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mNode feature dim changing\\033[0m'.rjust(40, ' '), graphs[0].ndata[list(graphs[0].ndata.keys())[0]].shape[1], hidden_features, out_features)\n",
    "        # opt = torch.optim.Adam(model.parameters())\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        epoch_losses = {'train': [], 'val': [], 'test': []}\n",
    "\n",
    "\n",
    "        ''' Training '''\n",
    "        for epoch in range(args.epochs):\n",
    "            train_loss = 0\n",
    "            for ibatch, (batched_graph, labels) in enumerate(train_dataloader):\n",
    "                # plot_Graph(batched_graph)\n",
    "                node_features = batched_graph.ndata['feature']\n",
    "                pred = model(batched_graph, node_features.float())\n",
    "                loss = loss_func(pred, labels.float())\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                train_loss += loss.detach().item()\n",
    "\n",
    "            ''' The loss per epoch for all batch is the average of losses per batch in this epoch '''\n",
    "            train_loss /= (ibatch + 1)\n",
    "\n",
    "            ''' Store loss per epoch '''\n",
    "            epoch_losses['train'].append(train_loss)\n",
    "\n",
    "            ''' Evaluate validation loss '''\n",
    "            for ibatch, (batched_graph, labels) in enumerate(val_dataloader):\n",
    "                node_features = batched_graph.ndata['feature']\n",
    "                pred = model(batched_graph, node_features.float())\n",
    "                val_loss = loss_func(pred, labels.float()).detach().item()\n",
    "                epoch_losses['val'].append(val_loss)\n",
    "                assert(ibatch == 0)\n",
    "\n",
    "            if epoch < 20:\n",
    "                print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mepoch, loss, val_loss:\\033[0m'.rjust(40, ' '),  epoch+1, '|', train_loss, '|', val_loss)\n",
    "            elif args.epochs > 100 and epoch % 100 == 0:\n",
    "                print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mepoch, loss, val_loss:\\033[0m'.rjust(40, ' '),  epoch+1, '|', train_loss, '|', val_loss)\n",
    "            elif args.epochs > 1000 and epoch % 1000 == 0:\n",
    "                print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mepoch, loss, val_loss:\\033[0m'.rjust(40, ' '),  epoch+1, '|', train_loss, '|', val_loss)\n",
    "            elif epoch == args.epochs - 1:\n",
    "                print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mepoch, loss, val_loss:\\033[0m'.rjust(40, ' '),  epoch+1, '|', train_loss, '|', val_loss)\n",
    "\n",
    "            ''' Early stopping '''\n",
    "            early_stopping = EarlyStopping(patience=10, verbose=False, path=gmodel_name+'.checkpoint')\n",
    "            # early_stopping needs the validation loss to check if it is decreasing, \n",
    "            # and if it is, it will make a checkpoint of the current model\n",
    "            early_stopping(val_loss, model)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mepoch, loss, val_loss:\\033[0m'.rjust(40, ' '),  epoch+1, '|', train_loss, '|', val_loss)\n",
    "                print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mEarly stop at:\\033[0m'.rjust(40, ' '),  epoch)\n",
    "                break\n",
    "    \n",
    "        ''' Load the last checkpoint with the best model. '''\n",
    "        model.load_state_dict(torch.load(gmodel_name+'.checkpoint'))\n",
    "\n",
    "        if path.exists(gmodel_name):\n",
    "            print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mRemove model:\\033[0m'.rjust(40, ' '),  gmodel_name)\n",
    "            remove(gmodel_name)\n",
    "\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mSave model:\\033[0m'.rjust(40, ' '),  gmodel_name)\n",
    "        torch.save(model, gmodel_name)\n",
    "\n",
    "    else: # args.no_training\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '\\033[92m[INFO]\\033[0m', '\\033[92mSkip training and load\\033[0m'.rjust(40, ' '),  gmodel_name)\n",
    "        model = torch.load(gmodel_name)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ''' Final Evaluate train loss '''\n",
    "        train_pred = []\n",
    "        train_labels = []\n",
    "        for ibatch, (batched_graph, labels) in enumerate(train_dataloader):\n",
    "            node_features = batched_graph.ndata['feature']\n",
    "            pred = model(batched_graph, node_features.float()).detach()\n",
    "            # print('zhangr train pred', pred.shape, pred)\n",
    "            # print('zhangr train labels', labels.shape, labels)\n",
    "            train_pred.append(pred)\n",
    "            train_labels.append(labels)\n",
    "        train_pred = torch.cat(train_pred).numpy()\n",
    "        train_labels = torch.cat(train_labels).numpy()\n",
    "\n",
    "        ''' Final Evaluate val loss '''\n",
    "        val_pred = []\n",
    "        val_labels = []\n",
    "        for ibatch, (batched_graph, labels) in enumerate(train_dataloader):\n",
    "            node_features = batched_graph.ndata['feature']\n",
    "            pred = model(batched_graph, node_features.float()).detach()\n",
    "            val_pred.append(pred)\n",
    "            val_labels.append(labels)\n",
    "            assert(ibatch == 0)\n",
    "        val_pred = torch.cat(val_pred).numpy()\n",
    "        val_labels = torch.cat(val_labels).numpy()\n",
    "\n",
    "\n",
    "        ''' Evaluate test loss '''\n",
    "        test_pred = []\n",
    "        test_labels = []\n",
    "        for ibatch, (batched_graph, labels) in enumerate(test_dataloader):\n",
    "            node_features = batched_graph.ndata['feature']\n",
    "            pred = model(batched_graph, node_features.float()).detach()\n",
    "            if not args.no_training:\n",
    "                test_loss = loss_func(pred, labels.float()).detach().item()\n",
    "                epoch_losses['test'].append(test_loss)\n",
    "            test_pred.append(pred)\n",
    "            test_labels.append(labels)\n",
    "            assert(ibatch == 0)\n",
    "        test_pred = torch.cat(test_pred).numpy()\n",
    "        test_labels = torch.cat(test_labels).numpy()\n",
    "\n",
    "    if not args.no_training:\n",
    "        print('zhangr loss', len(epoch_losses['train']), len(epoch_losses['val']))\n",
    "        plot_loss(epoch_losses['train'], epoch_losses['val'], starting = 0)\n",
    "    plot_ROC(test_labels, test_pred, train_labels, train_pred, val_labels, val_pred)\n",
    "    plot_response(test_labels, test_pred, train_labels, train_pred, val_labels, val_pred, scaling = '')\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \"\"\"Get arguments from command line.\"\"\"\n",
    "    parser = ArgumentParser(description='\\033[92mGNN training.\\033[0m')\n",
    "\n",
    "    parser.add_argument('--bs', type=int, default=10, help='Batch size (defult: %(default)s).')\n",
    "    parser.add_argument('--overwrite', type=bool, default=False, help='Overwrite if any to cached file (defult: %(default)s).')\n",
    "    parser.add_argument('--epochs', type=int, default=200, help='Epochs (defult: %(default)s).')\n",
    "    parser.add_argument('--module', type=str, choices=['sage', 'gat', 'agnnconv', 'sgc'], default='sgc', help='Network module')\n",
    "    parser.add_argument('--nevent', type=int, default=None, help='Number of graphs from events to create (defult: %(default)s).')\n",
    "    parser.add_argument('--connection', type=str, default='biadj_pt_y_phi', choices = ['bifully', 'biadj_pt_y_phi', 'biadj_pt_y', 'biadj_pt_phi', 'biadj_y_phi'], help='Type of edge connections (defult: %(default)s).')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    job1(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
